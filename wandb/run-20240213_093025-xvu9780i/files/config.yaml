wandb_version: 1

_wandb:
  desc: null
  value:
    python_version: 3.9.0
    cli_version: 0.16.3
    framework: huggingface
    huggingface_version: 4.37.2
    is_jupyter_run: false
    is_kaggle_kernel: false
    start_time: 1707816625.765641
    t:
      1:
      - 1
      - 11
      - 37
      - 41
      - 49
      - 55
      - 63
      - 71
      - 82
      - 83
      - 105
      2:
      - 1
      - 11
      - 37
      - 41
      - 49
      - 55
      - 63
      - 71
      - 82
      - 83
      - 105
      3:
      - 13
      - 23
      4: 3.9.0
      5: 0.16.3
      6: 4.37.2
      8:
      - 5
      13: linux-x86_64
_cfg_dict:
  desc: null
  value:
    data_root: /data/data
    data:
      type: canva
      root: images
      image_list_json:
      - data_info.json
      transform: default_train
      load_vae_feat: false
    image_size: 512
    train_batch_size: 64
    eval_batch_size: 16
    use_fsdp: false
    valid_num: 0
    model: PixArt_XL_2
    aspect_ratio_type: null
    multi_scale: false
    lewei_scale: 1.0
    num_workers: 4
    train_sampling_steps: 1000
    eval_sampling_steps: 200
    model_max_length: 512
    lora_rank: 4
    num_epochs: 200
    gradient_accumulation_steps: 1
    grad_checkpointing: true
    gradient_clip: 0.01
    gc_step: 1
    auto_lr:
      rule: sqrt
    optimizer:
      type: AdamW
      lr: 2.0e-05
      weight_decay: 0.03
      eps: 1.0e-10
      constructor: MyOptimizerConstructor
      paramwise_cfg:
        custom_keys: {}
    lr_schedule: constant
    lr_schedule_args:
      num_warmup_steps: 1000
    save_image_epochs: 1
    save_model_epochs: 1
    save_model_steps: 2000
    sample_model_steps: 1000
    sample_posterior: true
    mixed_precision: bf16
    scale_factor: 0.18215
    ema_rate: 0.9999
    tensorboard_mox_interval: 50
    log_interval: 20
    cfg_scale: 4
    mask_type: 'null'
    num_group_tokens: 0
    mask_loss_coef: 0.0
    load_mask_index: false
    vae_pretrained: /pyy/yuyang_blob/pyy/code/PixArt-alpha-canva/output/pretrained_models/sd-vae-ft-ema
    load_from: /pyy/yuyang_blob/pyy/code/PixArt-alpha-canva/output/PixArt-XL-2-512x512.pth
    resume_from:
      checkpoint: null
      load_ema: false
      resume_optimizer: true
      resume_lr_scheduler: true
    snr_loss: false
    work_dir: output/bf16_snr_offset
    s3_work_dir: null
    seed: 43
    window_block_indexes: []
    window_size: 0
    use_rel_pos: false
    fp32_attention: false
    use_flash_attn: false
    noise_offset: 0.1
    zero_snr: true
_filename:
  desc: null
  value: /pyy/yuyang_blob/pyy/code/PixArt-alpha-canva/configs/pixart_config/PixArt_xl2_img512_design.py
_text:
  desc: null
  value: '/pyy/yuyang_blob/pyy/code/PixArt-alpha-canva/configs/PixArt_xl2_design.py

    data_root = ''/data/data''

    data = dict(type=''InternalData'', root=''images'', image_list_json=[''data_info.json''],
    transform=''default_train'', load_vae_feat=True)

    image_size = 256  # the generated image resolution

    train_batch_size = 32

    eval_batch_size = 16

    use_fsdp=False   # if use FSDP mode

    valid_num=0      # take as valid aspect-ratio when sample number >= valid_num


    # model setting

    model = ''PixArt_XL_2''

    aspect_ratio_type = None         # base aspect ratio [ASPECT_RATIO_512 or ASPECT_RATIO_256]

    multi_scale = False     # if use multiscale dataset model training

    lewei_scale = 1.0    # lewei_scale for positional embedding interpolation

    # training setting

    num_workers=4

    train_sampling_steps = 1000

    eval_sampling_steps = 250

    model_max_length = 120

    lora_rank = 4


    num_epochs = 80

    gradient_accumulation_steps = 1

    grad_checkpointing = False

    gradient_clip = 1.0

    gc_step = 1

    auto_lr = dict(rule=''sqrt'')


    # we use different weight decay with the official implementation since it results
    better result

    optimizer = dict(type=''AdamW'', lr=1e-4, weight_decay=3e-2, eps=1e-10)

    lr_schedule = ''constant''

    lr_schedule_args = dict(num_warmup_steps=500)


    save_image_epochs = 1

    save_model_epochs = 1

    save_model_steps=2000

    sample_model_steps=1000


    sample_posterior = True

    mixed_precision = ''fp16''

    scale_factor = 0.18215

    ema_rate = 0.9999

    tensorboard_mox_interval = 50

    log_interval = 50

    cfg_scale = 4

    mask_type=''null''

    num_group_tokens=0

    mask_loss_coef=0.

    load_mask_index=False    # load prepared mask_type index

    # load model settings

    vae_pretrained = "/cache/pretrained_models/sd-vae-ft-ema"

    load_from = None

    resume_from = dict(checkpoint=None, load_ema=False, resume_optimizer=True, resume_lr_scheduler=True)

    snr_loss=False


    # work dir settings

    work_dir = ''/cache/exps/''

    s3_work_dir = None


    seed = 43


    /pyy/yuyang_blob/pyy/code/PixArt-alpha-canva/configs/pixart_config/PixArt_xl2_img512_design.py

    _base_ = [''../PixArt_xl2_design.py'']

    data = dict(type=''canva'', load_vae_feat=False)


    image_size = 512


    # model setting

    window_block_indexes=[]

    window_size=0

    use_rel_pos=False

    model = ''PixArt_XL_2''

    fp32_attention = False #True

    load_from = False

    vae_pretrained = "/pyy/yuyang_blob/pyy/code/PixArt-alpha-canva/output/pretrained_models/sd-vae-ft-ema"

    lewei_scale = 1.0

    model_max_length=512

    use_flash_attn= False


    # training setting

    use_fsdp= False   # if use FSDP mode

    num_workers=4

    train_batch_size = 64 # 32

    num_epochs = 200 # 3

    gradient_accumulation_steps = 1

    grad_checkpointing = True

    gradient_clip = 0.01

    optimizer = dict(type=''AdamW'', lr=2e-5, weight_decay=3e-2, eps=1e-10)

    lr_schedule_args = dict(num_warmup_steps=1000)

    noise_offset=0.1

    zero_snr = True

    mixed_precision = ''bf16''


    eval_sampling_steps = 200

    log_interval = 20

    save_model_epochs=1

    work_dir = ''output/bf16_with_snr_offset''

    '
